{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "551b1694-97e0-4659-b537-0e7c932b020e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, pickle, shutil, random, PIL\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from torch.utils.data import DataLoader,random_split,Dataset, ConcatDataset ,SubsetRandomSampler \n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from training_utils import *\n",
    "from cam_loss_training_utils import *\n",
    "from classification_models import *\n",
    "from focal_loss_with_smoothing import *\n",
    "\n",
    "from torchcam.methods import SmoothGradCAMpp, LayerCAM, GradCAM,CAM\n",
    "from torchcam.utils import overlay_mask\n",
    "from torchvision.transforms.functional import to_pil_image, resize\n",
    "from torch.nn.functional import softmax, interpolate\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09ed1571-27e3-469f-b53b-b5038ca1aef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "DFNAME = 'MB_34_Layer_1_L2_freezeb5'\n",
    "device = torch.device('cuda:0')\n",
    "criterion1 = FocalLossWithSmoothing(num_classes = 2, gamma=2, lb_smooth = 0.1)\n",
    "criterion2 =  nn.MSELoss()\n",
    "# criterion1 = nn.CrossEntropyLoss()\n",
    "\n",
    "modelname = 'MB_34_Layer_1_L2_freezeb5'\n",
    "n_epochs = 50\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3bf8261-7fb8-4ffc-be8f-e63802ef73c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '/home/aminul/CVL/cs791_project/malignant_vs_benign/train_original/imgs/'\n",
    "test_dir = '/home/aminul/CVL/cs791_project/malignant_vs_benign/test/imgs/'\n",
    "val_dir = '/home/aminul/CVL/cs791_project/malignant_vs_benign/val/imgs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c130a74d-8412-411a-b5c5-fe25b92a3147",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (1024,768)\n",
    "train_set_whole = ImageFolder(train_dir,transform = transforms.Compose([\n",
    "    # v2.Grayscale(1),\n",
    "    v2.RandomVerticalFlip(0.5),\n",
    "    v2.RandomHorizontalFlip(0.5),\n",
    "    v2.RandomRotation(15),\n",
    "    v2.RandomAutocontrast(0.5),\n",
    "    # v2.ColorJitter(brightness=0.4, contrast=0.2, saturation=0.3, hue=0.1),\n",
    "    # v2.RandomChannelPermutation(),\n",
    "    v2.RandomAdjustSharpness(2,0.5),\n",
    "    # v2.RandomAutocontrast(0.5),\n",
    "    v2.Resize(size),\n",
    "    # transforms.GaussianBlur(kernel_size=3),\n",
    "    # transforms.RandomRotation(30),\n",
    "    v2.ToTensor(),\n",
    "]))\n",
    "\n",
    "val_set = ImageFolder(val_dir,transform = transforms.Compose([\n",
    "    # v2.Grayscale(1),\n",
    "    v2.Resize(size),\n",
    "    v2.ToTensor(),\n",
    "]))\n",
    "\n",
    "test_set = ImageFolder(test_dir,transform = transforms.Compose([\n",
    "    # v2.Grayscale(1),\n",
    "    v2.Resize(size),\n",
    "    v2.ToTensor(),\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17c1879f-b76a-4ca7-ad4a-ae092021aced",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set_whole, batch_size=batch_size, shuffle=True, num_workers = 4)\n",
    "valid_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers = 4)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers = 4)\n",
    "test_loader_2 = DataLoader(test_set, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4cf38f6-40e2-4e47-b7dd-070e7ae41335",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 67/67 [00:03<00:00, 17.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2]  loss: [0.19] Test Accuracy [86.57] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "new_model =custom_rn(2).to(device)\n",
    "\n",
    "checkpoint = torch.load('./checkpoint/'+modelname+'model.pth.tar',map_location=torch.device('cpu'))\n",
    "new_model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "_, _, y, y_pred = best_test(new_model,test_loader_2,criterion1,optim,device,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e421cc24-b0f5-4845-9669-6509093c9dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_benign_img_names = os.listdir(\"/home/aminul/CVL/cs791_project/malignant_vs_benign/test/imgs/0_benign/\")\n",
    "test_malignant_img_names = os.listdir(\"/home/aminul/CVL/cs791_project/malignant_vs_benign/test/imgs/1_malignant/\")\n",
    "\n",
    "train_benign_img_names = os.listdir(\"/home/aminul/CVL/cs791_project/malignant_vs_benign/train_original/imgs/0_aug_benign/\")\n",
    "train_malignant_img_names = os.listdir(\"/home/aminul/CVL/cs791_project/malignant_vs_benign/train_original/imgs/1_aug_malignant/\")\n",
    "\n",
    "val_benign_img_names = os.listdir(\"/home/aminul/CVL/cs791_project/malignant_vs_benign/val/imgs/0_benign/\")\n",
    "val_malignant_img_names = os.listdir(\"/home/aminul/CVL/cs791_project/malignant_vs_benign/val/imgs/1_malignant/\")\n",
    "\n",
    "\n",
    "all_imgs_names_val = val_benign_img_names+val_malignant_img_names\n",
    "\n",
    "all_imgs_names_test = test_benign_img_names+test_malignant_img_names\n",
    "\n",
    "all_imgs_names_train = train_benign_img_names+train_malignant_img_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "853de9ae-c625-4986-a6ae-9b7de5c944e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels,train_labels, val_labels = [],[],[]\n",
    "\n",
    "for i in range(len(test_benign_img_names)):\n",
    "    test_labels.append(0)\n",
    "    \n",
    "for i in range(len(test_malignant_img_names)):\n",
    "    test_labels.append(1)\n",
    "    \n",
    "    \n",
    "for i in range(len(val_benign_img_names)):\n",
    "    val_labels.append(0)\n",
    "    \n",
    "for i in range(len(val_malignant_img_names)):\n",
    "    val_labels.append(1)\n",
    "    \n",
    "\n",
    "for i in range(len(train_benign_img_names)):\n",
    "    train_labels.append(0)\n",
    "    \n",
    "for i in range(len(train_malignant_img_names)):\n",
    "    train_labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c8f0224-8e45-4035-b6e9-fc8c34044ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_dir = '/home/aminul/CVL/cs791_project/malignant_vs_benign/test/combined_test_imgs/'\n",
    "# mask_dir = '/home/aminul/CVL/cs791_project/malignant_vs_benign/test/combined_masks/'\n",
    "\n",
    "# img_dir = '/home/aminul/CVL/cs791_project/malignant_vs_benign/train_original/combined_imgs/'\n",
    "# mask_dir = '/home/aminul/CVL/cs791_project/malignant_vs_benign/train_original/combined_masks/'\n",
    "\n",
    "img_dir = '/home/aminul/CVL/cs791_project/malignant_vs_benign/val/combined_imgs/'\n",
    "mask_dir = '/home/aminul/CVL/cs791_project/malignant_vs_benign/train_original/combined_masks/'\n",
    "\n",
    "img_filenames = os.listdir(img_dir)\n",
    "mask_filenames = os.listdir(mask_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5557a81-7bf7-4c52-bdec-e882ecbcdb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = v2.Compose([\n",
    "    v2.Resize((1024,768)),\n",
    "    v2.ToTensor(),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b69abe14-d820-4d14-9980-dfc1e210c32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sml_func(percentage):\n",
    "\n",
    "        # if percentage < 0.5:\n",
    "        #     return 3 ## small\n",
    "        # elif percentage >= 0.5 and percentage <3:\n",
    "        #     return 4\n",
    "        # elif percentage >= 3:\n",
    "        #     return 5\n",
    "        \n",
    "        \n",
    "        if percentage < 0.5:\n",
    "            return 3 ## small\n",
    "        elif percentage >= 0.5 and percentage <3:\n",
    "            return 4\n",
    "        elif percentage >= 0.5:\n",
    "            return 5\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16723434-07e5-4ec4-ac68-91b1d31536bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 10\n",
    "img_name, mask_values, percentage,sml_label = [],[],[],[]\n",
    "\n",
    "y_pred, y_true = [],[]\n",
    "\n",
    "\n",
    "for i in range(len(img_filenames)):\n",
    "\n",
    "    img = Image.open(img_dir + img_filenames[i])\n",
    "    ori_img = img.convert(\"RGB\") \n",
    "    \n",
    "    idx = all_imgs_names_val.index(img_filenames[i]) ### Change based on train or test\n",
    "    y_true.append(val_labels[idx]) ### Change based on train or test\n",
    "    \n",
    "    mask = Image.open(mask_dir + img_filenames[i])\n",
    "    mask = mask.convert('L')\n",
    "    threshold = 100\n",
    "    mask = mask.point(lambda p: p > threshold and 255)\n",
    "    mask_np = np.array(mask)\n",
    "    counted_values = np.count_nonzero(mask_np == 255)\n",
    "    mask_values.append(counted_values)\n",
    "    per = (counted_values / (mask_np.shape[0] * mask_np.shape[1]))*100\n",
    "    percentage.append(per)\n",
    "    sml_label.append(sml_func(per))\n",
    "    \n",
    "    \n",
    "    img1 = test_transform(copy.deepcopy(ori_img))\n",
    "    img1 = img1.to(device)\n",
    "    \n",
    "    new_model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs,_,_ = new_model(img1.unsqueeze(0))\n",
    "        _,predicted = torch.max(outputs.data,1)\n",
    "\n",
    "        y_pred.append(predicted.cpu().numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bec42786-0c05-473e-bacf-4fd02cc8f7f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] 1 3 0.00014133516503707223\n",
      "[0] 1 5 16.36893277102736\n",
      "[0] 1 3 0.0003406729426193207\n",
      "[0] 1 3 0.4520248611337578\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(y_pred)):\n",
    "    \n",
    "    if y_pred[i] != y_true[i]:\n",
    "        print(y_pred[i], y_true[i],sml_label[i],percentage[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a82e0305-91f3-4f32-ac21-daa7e69b1a76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8461538461538461"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0e48fad-5960-49d6-a76a-156e78aafdf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 4, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sml_label.count(3),sml_label.count(4),sml_label.count(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1aa2df-0a0e-4951-901c-1bf7a0940fba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e76703-ab15-46e1-a589-09d53d4c7d70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb91344-f1e4-44f1-a29e-bc0bba32a641",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0c2829-1b92-4a9f-b071-eeffa1ad0414",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cbab21-e5d4-4447-8f75-fde51952cb80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9be0e3a-6bd5-440d-99c2-5162de08aad5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2bf4f3-8fa1-4a49-a9ea-4a14551c4c91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab06b47-310e-4be1-94d2-d61fda81b5fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82203bc0-0f2f-48f2-90ec-2e8c35d41389",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c353ddca-e79b-4773-896c-3b6a3d5085ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ccd825-8c7d-4e75-8872-2fcbb82f7232",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b42154-c4bf-4583-b510-da19526692fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
