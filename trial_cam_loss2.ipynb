{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bf4db6-c976-4ae7-8892-95862b29dd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, pickle, shutil, random, PIL\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from torch.utils.data import DataLoader,random_split,Dataset, ConcatDataset ,SubsetRandomSampler \n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from training_utils import *\n",
    "from cam_loss_training_utils import *\n",
    "from classification_models import *\n",
    "from focal_loss_with_smoothing import *\n",
    "\n",
    "from torchcam.methods import SmoothGradCAMpp, LayerCAM, GradCAM,CAM\n",
    "from torchcam.utils import overlay_mask\n",
    "from torchvision.transforms.functional import to_pil_image, resize\n",
    "from torch.nn.functional import softmax, interpolate\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a303ee24-6197-45c0-a047-42bf46fdf011",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "DFNAME = 'MB_34_Layer_1_L2_freezeb5'\n",
    "device = torch.device('cuda:0')\n",
    "criterion1 = FocalLossWithSmoothing(num_classes = 2, gamma=2, lb_smooth = 0.1)\n",
    "criterion2 =  nn.MSELoss()\n",
    "# criterion1 = nn.CrossEntropyLoss()\n",
    "\n",
    "modelname = 'MB_34_Layer_1_L2_freezeb5'\n",
    "n_epochs = 50\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a021513c-68e8-4afe-a650-b7b8482126d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '/home/aminul/CVL/cs791_project/malignant_vs_benign/train_original/imgs/'\n",
    "test_dir = '/home/aminul/CVL/cs791_project/malignant_vs_benign/test/imgs/'\n",
    "val_dir = '/home/aminul/CVL/cs791_project/malignant_vs_benign/val/imgs/'\n",
    "\n",
    "cbis_test_dir = '/home/aminul/CVL/cbis_ddsm_test/imgs/'\n",
    "\n",
    "# train_dir = '/home/aminul/CVL/cs791_project/mass_vs_non_mass/train_original/imgs/'\n",
    "# test_dir = '/home/aminul/CVL/cs791_project/mass_vs_non_mass/test_original/imgs/'\n",
    "# val_dir = '/home/aminul/CVL/cs791_project/mass_vs_non_mass/val_original/imgs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16473920-a891-4b2b-94e9-00d0bbdca9d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "size = (1024,768)\n",
    "train_set_whole = ImageFolder(train_dir,transform = transforms.Compose([\n",
    "    # v2.Grayscale(1),\n",
    "    v2.RandomVerticalFlip(0.5),\n",
    "    v2.RandomHorizontalFlip(0.5),\n",
    "    v2.RandomRotation(15),\n",
    "    # v2.RandomAutocontrast(0.5),\n",
    "    # v2.ColorJitter(brightness=0.4, contrast=0.2, saturation=0.3, hue=0.1),\n",
    "    # v2.RandomChannelPermutation(),\n",
    "    v2.RandomAdjustSharpness(2,0.5),\n",
    "    # v2.RandomAutocontrast(0.5),\n",
    "    v2.Resize(size),\n",
    "    # transforms.GaussianBlur(kernel_size=3),\n",
    "    # transforms.RandomRotation(30),\n",
    "    v2.ToTensor(),\n",
    "]))\n",
    "\n",
    "val_set = ImageFolder(val_dir,transform = transforms.Compose([\n",
    "    # v2.Grayscale(1),\n",
    "    v2.Resize(size),\n",
    "    v2.ToTensor(),\n",
    "]))\n",
    "\n",
    "test_set = ImageFolder(test_dir,transform = transforms.Compose([\n",
    "    # v2.Grayscale(1),\n",
    "    v2.Resize(size),\n",
    "    v2.ToTensor(),\n",
    "]))\n",
    "\n",
    "cbis_test_set = ImageFolder(cbis_test_dir,transform = transforms.Compose([\n",
    "    # v2.Grayscale(1),\n",
    "    v2.Resize(size),\n",
    "    v2.ToTensor(),\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d3fa96-f675-4af2-8b6d-93f8700b7a0c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_set_whole, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m valid_loader \u001b[38;5;241m=\u001b[39m DataLoader(\u001b[43mval_set\u001b[49m, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m      3\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(test_set, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m      4\u001b[0m test_loader_2 \u001b[38;5;241m=\u001b[39m DataLoader(test_set, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'val_set' is not defined"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_set_whole, batch_size=batch_size, shuffle=True, num_workers = 4)\n",
    "valid_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers = 4)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers = 4)\n",
    "test_loader_2 = DataLoader(test_set, batch_size=1, shuffle=False)\n",
    "\n",
    "test_loader_cbis = DataLoader(cbis_test_set, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa2d0ca-2dac-4b77-8eb8-e1a609eb2e33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = custom_rn(2).to(device)\n",
    "\n",
    "c = 0\n",
    "for name, param in model.named_parameters():\n",
    "    if name[3] == '5': ### 9 if we use ef5\n",
    "        print(1)\n",
    "        break\n",
    "    param.requires_grad = False\n",
    "    c +=  1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63175158-0cd3-4f7c-aedd-9ea3ec15ccd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(model.parameters(),lr=0.0002, weight_decay=5e-4)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optim, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "history = {'train_loss': [], 'valid_loss': [],'train_acc':[],'valid_acc':[],'cam_loss':[]}\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    # train_loss, train_acc = train_model(model,train_loader,criterion1,optim,None,device,epoch)\n",
    "    # valid_loss, valid_acc = test_model(model,valid_loader, criterion1, optim,modelname,device,epoch)\n",
    "    \n",
    "    train_loss, cam_loss, train_acc = cam_loss_train_model(model,train_loader,criterion1,criterion2,optim,None,device,epoch)  \n",
    "    valid_loss, valid_acc = cam_loss_test_model(model,valid_loader,criterion1,optim,modelname,device,epoch)\n",
    "    \n",
    "    scheduler.step(valid_loss)\n",
    "    \n",
    "    # test(model,test_loader,criterion,optim,filename,modelname,device,epoch)\n",
    "            \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['valid_loss'].append(valid_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['valid_acc'].append(valid_acc)\n",
    "    \n",
    "    history['cam_loss'].append(cam_loss)\n",
    "\n",
    "with open('./storage/' + DFNAME + '.pkl', 'wb') as f:\n",
    "    pickle.dump(history, f) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a3c1e5-fa35-4cb5-8463-ed4314e49d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model =custom_rn(2).to(device)\n",
    "\n",
    "checkpoint = torch.load('./checkpoint/'+modelname+'model.pth.tar',map_location=torch.device('cpu'))\n",
    "new_model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "_, _, y, y_pred = best_test(new_model,test_loader_2,criterion1,optim,device,1)\n",
    "\n",
    "_, _, y, y_pred = best_test(new_model,test_loader_cbis,criterion1,optim,device,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a20e01-6ee6-44cd-8c36-d6abe769a303",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6eaf29-f20d-46fe-b0d9-a50440406c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_recall_fscore_support(y, y_pred, average= 'weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55777c82-68a7-4b1d-a766-f051f31554c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prob = torch.tensor(y_prob)\n",
    "# y_true = torch.tensor(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09747fb0-5793-439a-8581-44f4530ebbe7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sklearn import metrics\n",
    "\n",
    "# fpr, tpr, thresholds = metrics.roc_curve(y_true.numpy(), prob.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ef8f5d-a2ca-4304-876d-411964a780d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# roc_auc = metrics.auc(fpr, tpr)\n",
    "# print(roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8c5fee-87c4-4b6a-9fce-a56aa2b0fb8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='example estimator')\n",
    "# display.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d768a9f-92da-46ff-ac64-553198d0bc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _,y,y_pred = best_test(new_model,valid_loader,criterion1,optim,device,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512ad073-a0c5-4812-a944-fb9367091b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _,y,y_pred = best_test(new_model,train_loader,criterion1,optim,device,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a0c8c2-d8d3-491e-a868-26ed5b2559c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./storage/' + DFNAME + '.pkl', 'rb') as f:\n",
    "    data = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62bf2b3-ca5c-4611-9e3c-23f399aa1bc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8ec06b-e808-48df-b4c1-6727ef5f877f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(data['cam_loss'], label = 'CAM loss')\n",
    "# plt.plot(data['valid_loss'], label = 'Validation loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('CAM Loss vs Epoch Curve (CAAM)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149446ad-7b7f-4594-9290-761fcd72846e",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659814cf-dea6-4560-b8a2-2e67e7d6ab37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(data['train_loss'], label = 'Train loss')\n",
    "plt.plot(data['valid_loss'], label = 'Validation loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss vs Epoch Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ffbb78-30f4-4e3c-9233-963f35ca27d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(data['train_acc'], label = 'Train Accuracy')\n",
    "plt.plot(data['valid_acc'], label = 'Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy vs Epoch Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3460f84-c63b-40d4-8770-9cd3eee94cc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6066b1-aed3-4bff-8721-5952adf04667",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0dc88d-0eb6-4d8a-b865-a05e7f550f8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb7dd3e4-7dc8-4f43-b392-2f448fd3603f",
   "metadata": {},
   "source": [
    "### TTA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d49220-4da2-4df6-8123-c1ce38ba8516",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_benign_img_names = os.listdir(\"/home/aminul/CVL/cs791_project/malignant_vs_benign/test/imgs/0_benign/\")\n",
    "test_malignant_img_names = os.listdir(\"/home/aminul/CVL/cs791_project/malignant_vs_benign/test/imgs/1_malignant/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae11ed8e-04ff-4559-8dfc-aa08138ef4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_imgs_names = test_benign_img_names+test_malignant_img_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1542d33b-4ab9-4f3c-9ade-9de415c753ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "\n",
    "for i in range(len(test_benign_img_names)):\n",
    "    labels.append(0)\n",
    "    \n",
    "for i in range(len(test_malignant_img_names)):\n",
    "    labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3206b4-ecc7-46f1-a5b1-90f3d1eb7fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac949e7e-a67c-43b0-b334-7494a481f706",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = '/home/aminul/CVL/cs791_project/malignant_vs_benign/test/combined_test_imgs/'\n",
    "\n",
    "img_filenames = os.listdir(img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9de76a3-1fe3-458a-b3b0-5b090845f69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_score_func(ori_img, model, transforms):\n",
    "\n",
    "    img = transforms1(copy.deepcopy(ori_img))\n",
    "    img = img.to(device)\n",
    "\n",
    "    outputs,sl_map,ac_map = model(img.unsqueeze(0))\n",
    "\n",
    "    prob = F.softmax(outputs, dim=1)\n",
    "    top_p, top_class = prob.topk(1, dim = 1)\n",
    "    \n",
    "    return top_p, top_class, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573bf1ce-ea96-4b0e-9fe4-a0228c65d212",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms1 = v2.Compose([\n",
    "    v2.Resize(size),\n",
    "    v2.RandomHorizontalFlip(0.5),\n",
    "    # v2.RandomSolarize(threshold = 0.1,p=1),\n",
    "    v2.ToTensor(),\n",
    "    ])\n",
    "\n",
    "transforms2 = v2.Compose([\n",
    "    v2.Resize(size),\n",
    "    v2.RandomRotation(45),\n",
    "    # v2.RandomPosterize(bits=2,p=0.9),\n",
    "    v2.ToTensor(),\n",
    "    ])\n",
    "\n",
    "transforms3 = v2.Compose([\n",
    "    v2.Resize(size),\n",
    "    v2.RandomZoomOut(),\n",
    "    v2.ToTensor(),\n",
    "    ])\n",
    "\n",
    "transforms4 = v2.Compose([\n",
    "    v2.Resize(size),\n",
    "    v2.CenterCrop(size=(100, 100)),\n",
    "    v2.ToTensor(),\n",
    "    ])\n",
    "\n",
    "transforms5 = v2.Compose([\n",
    "    v2.Resize(size),\n",
    "    v2.RandomInvert(0.5),\n",
    "    v2.ToTensor(),\n",
    "    ])\n",
    "\n",
    "test_transform = v2.Compose([\n",
    "    v2.Resize(size),\n",
    "    v2.ToTensor(),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f3877f-6cc8-41e1-a7aa-dd6b30043575",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "# i = 42\n",
    "tta_pred, ori_label, ori_pred = [],[],[]\n",
    "for i in range(len(img_filenames)):\n",
    "    pred_per_img, score_per_img = [],[]\n",
    "\n",
    "    img = Image.open(img_dir + img_filenames[i])\n",
    "    ori_img = img.convert(\"RGB\") \n",
    "    idx = all_imgs_names.index(img_filenames[i])\n",
    "\n",
    "    ori_label.append(labels[idx])\n",
    "\n",
    "    top_p, top_class, _ = pred_score_func(ori_img, new_model, transforms1)\n",
    "    pred_per_img.append(top_class.item())\n",
    "    score_per_img.append(top_p.item())\n",
    "\n",
    "\n",
    "    top_p, top_class, _ = pred_score_func(ori_img, new_model, transforms2)\n",
    "    pred_per_img.append(top_class.item())\n",
    "    score_per_img.append(top_p.item())\n",
    "\n",
    "\n",
    "    top_p, top_class, _ = pred_score_func(ori_img, new_model, transforms3)\n",
    "    pred_per_img.append(top_class.item())\n",
    "    score_per_img.append(top_p.item())\n",
    "\n",
    "\n",
    "    top_p, top_class, _ = pred_score_func(ori_img, new_model, transforms4)\n",
    "    pred_per_img.append(top_class.item())\n",
    "    score_per_img.append(top_p.item())\n",
    "\n",
    "\n",
    "    top_p, top_class, _ = pred_score_func(ori_img, new_model, test_transform)\n",
    "    pred_per_img.append(top_class.item())\n",
    "    score_per_img.append(top_p.item())\n",
    "    \n",
    "    top_p, top_class, outputs = pred_score_func(ori_img, new_model, test_transform)\n",
    "    _,predicted = torch.max(outputs.data,1)\n",
    "    \n",
    "    \n",
    "    tta_pred.append((sum(pred_per_img)))\n",
    "    ori_pred.append(predicted.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110954e6-4375-4037-ad4e-f9a1d808feb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(tta_pred)):\n",
    "    print(tta_pred[i], ori_label[i], ori_pred[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f1259d-3772-4129-bef5-80e2375008d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
